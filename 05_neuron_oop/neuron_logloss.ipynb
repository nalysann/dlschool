{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"align: center;\">\n",
    "    <img align=center src=\"../img/dls_logo.jpg\" width=500 height=500>\n",
    "</p>\n",
    "\n",
    "<h1 style=\"text-align: center;\">\n",
    "    Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ\n",
    "</h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tWTTy4lrZca5"
   },
   "source": [
    "<h1 style=\"text-align: center;\">\n",
    "    Обучение нейрона с помощью функции потерь LogLoss\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VvIHYHMQZca7"
   },
   "source": [
    "## Напоминание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KU9Uf1w2Zca8"
   },
   "source": [
    "Почти любой алгоритм машинного обучения, решающий задачу **классификации** или **регрессии**, работает так:\n",
    "\n",
    "1. **Стадия инициализации**: задаются его **гиперпараметры**, то есть те величины, которые не \"выучиваются\" алгоритмом в процессе обучения самостоятельно.\n",
    "\n",
    "2. **Стадия обучения**: алгоритм запускается на данных, **обучаясь** на них и меняя свои **параметры** (не путать с *гипер*параметрами) каким-то определённым образом (например, с помощью *метода градиентного спуска* или *метода коррекции ошибки*), исходя из функции потерь (её называют *loss function*). Функция потерь, по сути, говорит, где и как ошибается модель.\n",
    "\n",
    "3. **Стадия предсказания**: модель готова, и теперь с её помощью можно делать **предсказания** на новых объектах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jdNcqz_wZca9"
   },
   "source": [
    "В данном ноутбуке будет решаться задача **бинарной классификации** с помощью нейрона:\n",
    "\n",
    "* **Входные данные**: матрица $X$ размера $(n, m)$ и столбец $y$ из нулей и единиц размера $(n, 1)$. Строкам матрицы соответствуют объекты, столбцам - признаки (то есть строка $i$ есть набор признаков (**признаковое описание**) объекта $X_i$).\n",
    "\n",
    "* **Выходные данные**: столбец $\\hat{y}$ из нулей и единиц размера $(n, 1)$ - предсказания алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_zlIW5BZcbB"
   },
   "source": [
    "## Нейрон с сигмоидой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QWwW32pCZcbC"
   },
   "source": [
    "Снова рассмотрим нейрон с сигмоидой, то есть:\n",
    "\n",
    "$$\n",
    "f(x) = \\sigma(x)=\\frac{1}{1+e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "mj5QizQIZcbC"
   },
   "source": [
    "На предыдущем занятии мы установили, что **обучение нейрона с сигмоидой с квадратичной функцией потерь**:  \n",
    "\n",
    "$$\n",
    "MSE(w, x) = \\frac{1}{n}\\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2 = \\frac{1}{n}\\sum_{i=1}^{n} (\\sigma(w \\cdot x_i) - y_i)^2\n",
    "$$    \n",
    "\n",
    "где $w \\cdot x_i$ - скалярное произведение, а $\\sigma(w \\cdot x_i) =\\frac{1}{1+e^{-w \\cdot x_i}} $ - сигмоида, **неэффективно**, то есть мы увидели, что даже за большое количество итераций нейрон предсказывает плохо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MTgNbravZcbE"
   },
   "source": [
    "Давайте ещё раз взглянем на формулу для градиентного спуска от функции потерь $L$ по весам нейрона:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial w} = \\frac{2}{n} X^T (\\sigma(w \\cdot X) - y)\\sigma(w \\cdot X)(1 - \\sigma(w \\cdot X))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ebNqrTV3ZcbF"
   },
   "source": [
    "А теперь смотрим на график сигмоиды:\n",
    "\n",
    "<img src='../img/neuron_1.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CSEF3EWVZcbJ"
   },
   "source": [
    "**Её значения: числа от 0 до 1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8oOJPcKhZcbJ"
   },
   "source": [
    "Если получше проанализировать формулу, то теперь можно заметить, что, поскольку сигмоида принимает значения между $0$ и $1$ (а значит ($1-\\sigma$) тоже принимает значения от $0$ до $1$), то мы умножаем $X^T$ на столбец $(\\sigma(w \\cdot X) - y)$ из чисел от $-1$ до $1$, а потом ещё на столбцы $\\sigma(w \\cdot X)$ и $(1 - \\sigma(w \\cdot X))$ из чисел от $0$ до $1$. Таким образом в лучшем случае $\\frac{\\partial{L}}{\\partial{w}}$ будет столбцом из чисел, порядок которых максимум $0.01$ (в среднем, понятно, что если сигмоида выдаёт все $0$, то будет $0$, если все $1$, то тоже $0$). После этого мы умножаем на шаг градиентного спуска, который обычно порядка $0.001$ или $0.01$ максимум. То есть мы вычитаем из весов числа порядка $\\sim 0.0001$. Медленновато спускаемся, не правда ли? Это называют **проблемой затухающих градиентов**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hHmUqm5xZcbK"
   },
   "source": [
    "Всё верно. В задачах классификации, в которых моделью является нейрон с сигмоидной функцией активации, предсказывающий \"вероятности\" принадлженостей к классам, почти никогда не используют квадратичную функцию потерь $L$. Вместо неё придумали другую прекрасную функцию потерь - **LogLoss**:\n",
    "\n",
    "$$\n",
    "LogLoss(\\hat{y}, y) = -\\frac{1}{n} \\sum_{i=1}^n y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i}) = -\\frac{1}{n} \\sum_{i=1}^n y_i \\log(\\sigma(w \\cdot x_i)) + (1 - y_i) \\log(1 - \\sigma(w \\cdot x_i))\n",
    "$$\n",
    "\n",
    "где, как и прежде, $y$ - вектор из истинных значений классов, а $\\hat{y}$ - вектор из предсказаний нейрона."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5DBEOhJmZcbN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_x5CX0RyZcbQ"
   },
   "outputs": [],
   "source": [
    "def loss(y_pred, y):\n",
    "    return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "raPTihU6ZcbT"
   },
   "source": [
    "Отметим, что сейчас речь идёт именно о **бинарной классификации** (на два класса), в многоклассовой классификации используется функция потерь под названием **кросс-энтропия**, которая является обобщением LogLoss'а на случай нескольких классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uz7UJCW3ZcbU"
   },
   "source": [
    "Почему же теперь всё будет лучше? Раньше была проблема умножения маленьких чисел в градиенте. Давайте посмотрим, что теперь:\n",
    "\n",
    "* Для веса $w_j$:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial Loss}{\\partial w_j} = \n",
    "  -\\frac{1}{n} \\sum_{i=1}^n \\left(\\frac{y_i}{\\sigma(w \\cdot x_i)} - \\frac{1 - y_i}{1 - \\sigma(w \\cdot x_i)}\\right)(\\sigma(w       \\cdot x_i))_{w_j}' = -\\frac{1}{n} \\sum_{i=1}^n \\left(\\frac{y_i}{\\sigma(w \\cdot x_i)} - \\frac{1 - y_i}{1 - \\sigma(w \\cdot       x_i)}\\right)\\sigma(w \\cdot x_i)(1 - \\sigma(w \\cdot x_i))x_{ij} =\n",
    "  $$\n",
    "  \n",
    "  $$\n",
    "  -\\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\sigma(w \\cdot x_i)\\right)x_{ij}\n",
    "  $$\n",
    "  \n",
    "* Градиент $Loss$'а по вектору весов - это вектор, $j$-ая компонента которого равна $\\frac{\\partial Loss}{\\partial w_j}$ (помним, что весов всего $m$):\n",
    "\n",
    "  $$\n",
    "  \\begin{align}\n",
    "      \\frac{\\partial Loss}{\\partial w} &= \\begin{bmatrix}\n",
    "          -\\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\sigma(w \\cdot x_i)\\right)x_{i1} \\\\\n",
    "          -\\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\sigma(w \\cdot x_i)\\right)x_{i2} \\\\\n",
    "          \\vdots \\\\\n",
    "          -\\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\sigma(w \\cdot x_i)\\right)x_{im}\n",
    "      \\end{bmatrix}\n",
    "   \\end{align}=\\frac{1}{n} X^T \\left(\\hat{y} - y\\right)\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По аналогии с нейроном выведем формулу для свободного члена (bias'а) $b$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial b} = \n",
    "-\\frac{1}{n} \\sum_{i=1}^n \\left(\\frac{y_i}{\\sigma(w \\cdot x_i + b)} - \\frac{1 - y_i}{1 - \\sigma(w \\cdot x_i + b)}\\right)(\\sigma(w \\cdot x_i + b))_{b}' = -\\frac{1}{n} \\sum_{i=1}^n \\left(\\frac{y_i}{\\sigma(w \\cdot x_i + b)} - \\frac{1 - y_i}{1 - \\sigma(w \\cdot x_i + b)}\\right)\\sigma(w \\cdot x_i + b)(1 - \\sigma(w \\cdot x_i + b))*1 =\n",
    "$$\n",
    "\n",
    "$$\n",
    "-\\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\sigma(w \\cdot x_i)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7HQALb8XZcbU"
   },
   "source": [
    "Получили новое правило для обновления. Заметим, что это в точности то правило обновления весов для градиентного спуска, которое мы использовали для перцептрона (только другая функция активации). Получается, что мы пришли к этому правилу \"по-честному\", сделав функцией активации сигмоиду и введя новую функию потерь - LogLoss, а когда реализовывали перцептрон, мы просто сказали (воспользовавшись **правилом Хебба**), что $f'(x)$ возьмём единицей, то есть тут имеет место интересная связь градиентного спуска и метода коррекции ошибки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Kr-edseZcbW"
   },
   "source": [
    "Отсюда очевидно, что код для нейрона с такой функцией потерь не будет ничем отличаться от кода для перцептрона, за исключением метода `self.activate` и самого подсчёта `loss`. Напишем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xL2gqg27ZcbW"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Сигмоидная функция\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ukZ-iJNRZcbY"
   },
   "source": [
    "Реализуем нейрон с функцией потерь LogLoss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TdnkoJDfZcba"
   },
   "outputs": [],
   "source": [
    "class Neuron:      \n",
    "    def __init__(self, w=None, b=0):\n",
    "        \"\"\"\n",
    "        param w - вектор весов\n",
    "        param b - смещение\n",
    "        \"\"\"\n",
    "\n",
    "        # пока что мы не знаем размер матрицы X, а значит не знаем, сколько будет весов\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        \n",
    "    def activate(self, x):\n",
    "        return sigmoid(x)\n",
    "        \n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"\n",
    "        Рассчитывает ответ нейрона при предъявлении набора объектов\n",
    "            param X - матрица объекты-признаки размера (n, m), каждая строка - отдельный объект\n",
    "            return - вектор размера (n,) из нулей и единиц с ответами нейрона\n",
    "        \"\"\"\n",
    "        \n",
    "        y_pred = self.activate(X @ self.w + self.b)\n",
    "        return y_pred\n",
    "    \n",
    "    def backward_pass(self, X, y, y_pred, lr=0.1):\n",
    "        \"\"\"\n",
    "        Обновляет значения весов нейрона\n",
    "            param X - матрица входов размера (n, m)\n",
    "            param y - вектор правильных ответов размера (n,)\n",
    "            param lr - \"скорость обучения\" (символ alpha в формулах выше)\n",
    "        \n",
    "        В этом методе ничего возвращать не нужно, только правильно\n",
    "        поменять веса с помощью градиентного спуска.\n",
    "        \"\"\"\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        \n",
    "        # осторожнее с порядком операций\n",
    "        self.w -= lr / n * X.T @ (y_pred - y)\n",
    "        self.b -= lr * np.mean(y_pred - y)\n",
    "\n",
    "    def fit(self, X, y, num_epochs=5000):\n",
    "        \"\"\"\n",
    "        Спускаемся в минимум\n",
    "            param X - матрица объектов размера (n, m)\n",
    "            param y - вектор правильных ответов размера (n,)\n",
    "            param num_epochs - количество итераций обучения\n",
    "            return - вектор значений функции потерь\n",
    "        \"\"\"\n",
    "        \n",
    "        # вектор размера (m,)\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        # смещение\n",
    "        self.b = 0\n",
    "        # значения функции потерь на различных итерациях обновления весов\n",
    "        losses = []\n",
    "        \n",
    "        for i in range(num_epochs):\n",
    "            # предсказания с текущими весами\n",
    "            y_pred = self.forward_pass(X)\n",
    "            # считаем функцию потерь с текущими весами\n",
    "            losses.append(loss(y_pred, y))\n",
    "            # обновляем веса в соответсвии с тем, где ошиблись раньше\n",
    "            self.backward_pass(X, y, y_pred)\n",
    "        \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-RQRPI50Zcbb"
   },
   "source": [
    "## Тестирование нейрона"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FBwpsQc0Zcbc"
   },
   "source": [
    "Протестируем нейрон, обученный с новой функцией потерь, на тех же данных, что и в предыдущем ноутбуке:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JVCPAAuXZcbd"
   },
   "source": [
    "#### Проверка `forward_pass`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 646,
     "status": "ok",
     "timestamp": 1539422634948,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -300
    },
    "id": "hbuAzv_YZcbe",
    "outputId": "95004e10-a72e-4aaf-9929-2589d8722bad"
   },
   "outputs": [],
   "source": [
    "w = np.array([1., 2.])\n",
    "b = 2.\n",
    "X = np.array([[1., 3.],\n",
    "              [2., 4.],\n",
    "              [-1., -3.2]])\n",
    "\n",
    "neuron = Neuron(w, b)\n",
    "y_pred = neuron.forward_pass(X)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eL6viEHNZcbi"
   },
   "source": [
    "#### Проверка `backward_pass`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_4F8zActZcbk"
   },
   "outputs": [],
   "source": [
    "y = np.array([1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 750,
     "status": "ok",
     "timestamp": 1539422636670,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -300
    },
    "id": "ILDcoLqwZcbm",
    "outputId": "a70db24d-e05a-4a05-d203-77576034aa80"
   },
   "outputs": [],
   "source": [
    "neuron.backward_pass(X, y, y_pred)\n",
    "\n",
    "print(neuron.w)\n",
    "print(neuron.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1ZRdKcfZcbq"
   },
   "source": [
    "Загрузим данные (яблоки и груши):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 878
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 996,
     "status": "error",
     "timestamp": 1539422637796,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -300
    },
    "id": "RWYeLd-hZcbq",
    "outputId": "343d6ee6-e401-437c-b0c7-2e1dff231edf"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/apples_pears.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ecudw87Zcbu"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wjc26FMiZcbz"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=data['target'], cmap='rainbow')\n",
    "plt.title('Яблоки и груши', fontsize=15)\n",
    "plt.xlabel('Симметричность', fontsize=14)\n",
    "plt.ylabel('Желтизна', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eaTnrIVUZcb1"
   },
   "source": [
    "Обозначим, что здесь признаки, а что - классы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 670,
     "status": "error",
     "timestamp": 1539422639700,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -300
    },
    "id": "zwVu_NsCZcb2",
    "outputId": "264561a6-089d-4256-bd5c-5aaa12ab769d"
   },
   "outputs": [],
   "source": [
    "# матрица объекты-признаки\n",
    "X = data.iloc[:, :2].values\n",
    "# классы (столбец из нулей и единиц)\n",
    "y = data['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JsuH5PUrZcb5"
   },
   "source": [
    "**Вывод функции потерь.** Функция потерь должна убывать и в итоге стать близкой к $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PBJ0ZH6SZcb6"
   },
   "outputs": [],
   "source": [
    "# может вызывать баги на старых версиях jupyter notebook\n",
    "# %%time\n",
    "\n",
    "neuron = Neuron()\n",
    "losses = neuron.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(losses)\n",
    "plt.title('Функция потерь', fontsize=15)\n",
    "plt.xlabel('Номер итерации', fontsize=14)\n",
    "plt.ylabel('$Loss(\\hat{y}, y)$', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1PcnRTaZcb9"
   },
   "source": [
    "Посмотрим, как нейрон классифицировал объекты из выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7bvaljbsZcb-"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=np.array(neuron.forward_pass(X) > 0.5).ravel(), cmap='spring')\n",
    "plt.title('Яблоки и груши', fontsize=15)\n",
    "plt.xlabel('Симметричность', fontsize=14)\n",
    "plt.ylabel('Желтизна', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELtGlYqBZccE"
   },
   "source": [
    "Видим, что с **LogLoss** в случае классификации работать лучше, чем с квадратичной функцией потерь (с $5000$ итераций и `lr=0.1` здесь лучше, чем с $5000$ итераций и `lr=0.1` в нейроне с **MSE**). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полезные ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Статья от Стэнфорда - http://cs231n.github.io/neural-networks-1/\n",
    "\n",
    "2. Хорошая статья про функции активации - https://www.jeremyjordan.me/neural-networks-activation-functions/\n",
    "\n",
    "3. Видео от Siraj Raval - https://www.youtube.com/watch?v=-7scQpJT7uo\n",
    "\n",
    "4. Современная статья про функции активации. Теперь на хайпе активация $swish(x) = x\\sigma (\\beta x)$: https://arxiv.org/pdf/1710.05941.pdf (кстати, при её поиске в некоторой степени использовался neural architecture search)\n",
    "\n",
    "5. **SeLU** - имеет очень интересные, доказанные с помощью теории вероятностей свойства: https://arxiv.org/pdf/1706.02515.pdf (да, в этой статье 102 страницы)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[seminar]neuron_logloss.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
