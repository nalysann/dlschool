{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wU-s4PgBFoRY"
   },
   "source": [
    "<p style=\"align: center;\">\n",
    "    <img align=center src=\"../img/dls_logo.jpg\" width=500 height=500>\n",
    "</p>\n",
    "\n",
    "<h1 style=\"text-align: center;\">\n",
    "    <b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b>\n",
    "</h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZRDXUuj7FoRo"
   },
   "source": [
    "# 1. Основные понятия и пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r42wjO9XFoRr"
   },
   "source": [
    "Машинное обучение - наука о восстановлении закономерностей по частным данным. Рассмотрим на примере, что под этим имеется в виду."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jjnobv6mFoRt"
   },
   "source": [
    "<img src=\"../img/ml_basics_farmer.jpg\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZhJJpOEqFoRx"
   },
   "source": [
    "Предположим, что вы - начинающий фермер-любитель, и занимаетесь выращиванием картофеля. Ваш огород разделён на несколько участков, и уже прошёл месяц с того момента, как вы посадили картофель. К сожалению, в этом году особенно много колорадских жуков, а оставшегося у Вас средства от вредителей хватит только на один из этих участков. Жуки очень очень его боятся, и трогать обработанный участок не будут. Однако все остальные картофельные кусты они съедят подчистую. Поэтому Вы хотите узнать, какой из участков принесёт вам больше всего килограмм картофеля."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C0AcmpOwFoRz"
   },
   "source": [
    "Иными словами, Вы хотите по каждому из участков научится определять, сколько картофеля он принесёт. Однако, к вашему сожалению, вы не имеете никакого понятия, от чего это может зависеть. Однако, к вашему счастью, в прошлом году вы, по какой-то причине, решили измерить площадь и число кустов картофеля на каждом из участков на соседнем огороде, а также записали, каков был урожай на каждом из них. Если бы вы могли, **используя эти данные, восстановить закон**, по которому площадь и число кустов на участке переходит в киллограмы картофеля, то ваш урожай был бы спасён!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0EVF3F4FFoR3"
   },
   "source": [
    "## 1.1 Термины"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aAoesGSCFoR7"
   },
   "source": [
    "Формулируя задачу в терминах машинного обучения, множество участков с картофелем (как наших, так и соседа) будет называться **пространством объектов**, и обозначаться через $\\mathbb{X}$ (обратите внимание на шрифт). А величина, которую мы хотим научиться определять - в данном случае, количество килограмм картофеля при сборе урожая - **целевой переменной**. Множество значений целевой переменной - в данном случае, это вещественные неотрицательные числа - обозначается через $\\mathbb{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "caTkMvr_FoR-"
   },
   "source": [
    "Множество объектов, для которых нам известно значение целевой переменной - в данном случае это множество участков на соседнем огороде - называются **обучающей выборкой**, для которой используется обозначение $X = \\{ x_1, \\ldots, x_n\\}$, где $x_i$ - это участок $i$. Соответственно, значения целевой переменной $y_1, \\ldots, y_n$ - объёмы урожая на каждом из этих участков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0YzrXv9KFoSB"
   },
   "source": [
    "Непонятно, как можно построить формальный закон, оперируя такими неясными с математической точки зрения объектами как участок с картофелем. Поэтому каждому объекту выборки соотносят набор параметров, который этот объект описывает, который называют **вектором признаков**. В нашем примере это площадь участка и число кустов на нём. Обычно объект отождествляют с его вектором признаков, поэтому будем считать, что $x_i$ - это набор признаков объекта $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4INRLM1SFoSC"
   },
   "source": [
    "Цель задачи машинного обучения - найти такую функцию $f$, переводящую вектор признаков в целевую переменную - площадь и число кустов в число киллограммов - что её значения будут \"наиболее близко\" приближать истинные значения целевой переменной. Такая функция $f$ называется **моделью**. Конечно, в идеале нам хотелось бы, чтобы $f$ всегда давала точный ответ, но такой точности почти никогда не получается добиться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Z44-L5aFoSE"
   },
   "source": [
    "Но что означает \"наиболее близко\"? Которая ошибается не больше, чем 1 киллограмм? Но таких функций может быть много. Какую из них выбрать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFwbgrROFoSG"
   },
   "source": [
    "Заметим, что судить о \"точности\" нашей модели мы можем только по обучающей выборке, так как только для неё мы знаем истинные ответы. Поэтому в машинном обучении используется подход, основанный на так называемой **функции потерь**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zncEp_xTFoSI"
   },
   "source": [
    "Функция потерь $L(f, X, y)$ даёт нам численную оценку \"точности\" нашей модели на выборке $X$. То есть, формально говоря, она сопоставляет паре из модели и выборки число. Очень часто используется так называемая **среднеквадратичная ошибка**:\n",
    "\n",
    "$$\n",
    "    L(f, X, y) = \\frac{1}{n}\\sum\\limits_{i=1}^{n} (f(x_i) - y_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DDUjdRBqFoSK"
   },
   "source": [
    "Весьма логично было бы взять такую функцию $f$, для которой значение функции потерь было бы минимальным. Но тут нас встречает ещё она проблема. В принципе, никто не мешает нам взять такую функцию $f$, что её значение на векторах из обучающей выборки будут в точности равны целевой переменной, а на всех остальных - $0$. Однако пользы от такой модели никакой, поскольку когда мы попытаемся предсказать с её помощью значение $y$ на новых данных, мы получим $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86nffGYnFoSN"
   },
   "source": [
    "Поэтому множество всех возможных моделей ограничивают каким-то семейством (множеством) $\\mathcal{A}$, в котором потом и ищут наилучшую модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rWuib9DSFoSQ"
   },
   "source": [
    "Таким образом, в общей форме задача машинного обучения звучит так: найти такую функцию $f$ из $\\mathcal{A}$, что значение функции потерь $L$ на выборке $X, y$ было бы минимальным, или\n",
    "\n",
    "$$\n",
    "    f_* = \\arg \\min_{f \\in \\mathcal{A}} L(f, X, y)\n",
    "$$\n",
    "\n",
    "Задачи такого рода называются задачами оптимизации, или минимизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OzcCkLBUFoST"
   },
   "source": [
    "Обычно семейство $\\mathcal{A}$ можно параметризовать, то есть поставить в соответствие каждой модели из $\\mathcal{A}$ какое-то число или вектор из чисел $w$. Тогда соответствующая задача оптимизации будет выглядеть следующим образом:\n",
    "\n",
    "$$\n",
    "    w_* = \\arg \\min_{w \\in \\mathbb{W}} L(f_w, X, y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_3cId_9vFoSV"
   },
   "source": [
    "## 1.2 Пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jhdwNf05FoSa"
   },
   "source": [
    "Разберём это на нашем примере. В нашем примере вектор признаков - это пара из площади и числа кустов, которые обозначим за $s_i$ и $k_i$. В качестве функции потерь будем рассматривать среднеквадратичную ошибку. Наконец, в качестве множества $\\mathcal{A}$ будем использовать **линейные модели**, то есть функции вида:\n",
    "\n",
    "$$\n",
    "    f(s, k) = a + bs + ck\n",
    "$$\n",
    "\n",
    "где $a, b, c$ - произвольные вещественные числа.\n",
    "\n",
    "Заметим, что на таком множестве можно ввести очень естественную параметризацию, а именно $w \\in \\mathbb{R}^3$:\n",
    "\n",
    "$$\n",
    "    f_w(s, k) = w_0 + w_1s + w_2k\n",
    "$$\n",
    "\n",
    "Отметим, что зачастую параметр $w$ записывают как аргумент для функции $f$:\n",
    "\n",
    "$$\n",
    "    f(w, s, k) = w_0 + w_1s + w_2k\n",
    "$$\n",
    "\n",
    "Пожалуйста, будте внимательны и не путайте параметры и аргументы функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b9xku2eyFoSd"
   },
   "source": [
    "Теперь запишем это в виде задачи оптимизации:\n",
    "\n",
    "$$\n",
    "    w_* = \\arg \\min_{w \\in \\mathbb{R}^3} L(f_w, X, y) = \n",
    "    \\arg \\min_{w \\in \\mathbb{R}^3}  \\frac{1}{n} \\sum\\limits_{i=1}^{n}(f_w(x_i) - y_i)^2 =\n",
    "    \\arg \\min_{w \\in \\mathbb{R}^3}  \\frac{1}{n} \\sum\\limits_{i=1}^{n}(w_0 + w_1s + w_2k - y_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vYEajIvaFoSf"
   },
   "source": [
    "### 1.2.1 А как найти минимум?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7cI0laFGFoSi"
   },
   "source": [
    "В самом деле, всё что нас отделяет от успеха - это, собственно, поиск минимума. К сожалению, в общем случае выписать явную формулу не удаётся, и приходится использовать приближённые методы - такие, как градиентный спуск. Вообще, методы оптимизации - это отдельная большая наука. Но, тем не менее, если достаточно сильно упростить задачу, решить её сможет любой школьник."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "76fG7N42FoSj"
   },
   "source": [
    "###  1.2.2 Очень простой пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sepCjXErFoSm"
   },
   "source": [
    "Упростим постановку задачи, упростив множество $\\mathcal{A}$. В принципе, никто не запрещает нам игнорировать один из наших признаков - скажем, число кустов. Также избавимся от свободного члена $w_0$. Тогда наша функция приобретёт вид:\n",
    "\n",
    "$$\n",
    "f_w(s) = ws, w \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "Подставим это в функцию потерь:\n",
    "\n",
    "$$\n",
    "L(w, X) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} (ws_i - y_i)^2\n",
    "$$\n",
    "\n",
    "Раскроем скобки:\n",
    "$$\n",
    "L(w, X) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} (s_i^2 w^2 - 2 y_i s_i w + y_i^2)\n",
    "$$\n",
    "\n",
    "Перегруппируем слагаемые:\n",
    "\n",
    "$$\n",
    "L(w, X) = \\frac{1}{n}( \\sum\\limits_{i=1}^{n} (s_i^2) w^2 - \\sum\\limits_{i=1}^{n}(2 y_i s_i) w + \\sum\\limits_{i=1}^{n} (y_i^2))\n",
    "$$\n",
    "\n",
    "Теперь обозначим:\n",
    "$$\n",
    "a = \\frac{1}{n} \\sum\\limits_{i=1}^{n} (s_i^2),\n",
    "b = -\\frac{1}{n} \\sum\\limits_{i=1}^{n}(2 y_i s_i),\n",
    "c = \\frac{1}{n} \\sum\\limits_{i=1}^{n} (y_i^2)\n",
    "$$\n",
    "\n",
    "Таким образом:\n",
    "\n",
    "$$\n",
    "L(w, X) = aw^2 + bw + c\n",
    "$$\n",
    "\n",
    "Как мы видим, мы получили обычный квадратный трёхчлен от $w$. Как всем известно, если $a > 0$ (что, как можно видеть, в данном случае выполняется), то его минимум достигается в точке:\n",
    "\n",
    "$$\n",
    "w_* = \\frac{-b}{2a} = \\frac{\\frac{1}{n} \\sum\\limits_{i=1}^{n}2 y_i s_i}{\\frac{2}{n} \\sum\\limits_{i=1}^{n} s_i^2} = \n",
    "\\frac{\\sum\\limits_{i=1}^{n}y_i s_i}{\\sum\\limits_{i=1}^{n} s_i^2} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AhmQVUPSFoSn"
   },
   "source": [
    "К сожалению, очевидно, что сколь-нибудь нетривиальную зависимость выразить таким способов не выдет, однако даже это хороший пример того, как нужно менять параметры модели, чтобы она предсказывала оптимально (с точки зрения выбранной функции потерь). \n",
    "\n",
    "То есть суть **обучения** состоит в том, чтобы **изменить параметры** модели из выбранного семейства таким образом, чтобы она предсказывала **оптимально** с точки зрения **выбранной функции потерь**, для этого используется определённый **алгоритм оптимизации**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H2K7LDZ1FoSo"
   },
   "source": [
    "# 2. Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42u4CQbjFoSp"
   },
   "source": [
    "Рассмотренная нами задача относится к **задаче регрессии** - то есть её целевая переменная принимает значения в $\\mathbb{R}$. Кроме неё существует множество других задач, например задача классификации.\n",
    "\n",
    "Кроме того, эта задача является примером задачи **обучения с учителем** - то есть, нам даны обучающие примеры с известными значениями целевой переменной. Задачи, в которых целевые переменные неизвестны, относятся к задачам **обучения без учителя**.\n",
    "\n",
    "Мир машинного обучения чрезвычайно разнообразен, поэтому в нашем курсе мы сможем познакомить вас лишь с его небольшой частью, перед тем, как углубиться в нейронные сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SFYozOuFoSx"
   },
   "source": [
    "# 3. Полезные ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LPgBJ6-WFoSz"
   },
   "source": [
    "* [Больше о разных направлениях машинного обучения](http://www.machinelearning.ru/wiki/index.php?title=Машинное_обучение)\n",
    "\n",
    "* [Лекции Евгения Соколова](https://github.com/esokolov/ml-course-msu/tree/master/ML15/lecture-notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Практическое занятие"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Данные**](https://www.kaggle.com/ramamet4/app-store-apple-data-set-10k-apps) - информация о приложениях из AppStore.\n",
    "\n",
    "Поставим регрессионную задачу - предсказать рейтинг приложения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с самой важной части - посмотрим на данные. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим данные и посмотрим на небольшую часть\n",
    "data = pd.read_csv('data/AppleStore.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим фичи из датасета и поделим их на числовые и категориальные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\n",
    "    'size_bytes',\n",
    "    'price',\n",
    "    'rating_count_tot',\n",
    "    'rating_count_ver',\n",
    "    'sup_devices.num',\n",
    "    'ipadSc_urls.num',\n",
    "    'lang.num',\n",
    "    'cont_rating', # эта фича - не числовая, а порядковая, но мы все равно возьмем ее как числовую для удобства\n",
    "]\n",
    "\n",
    "cat_cols = [\n",
    "    'currency',\n",
    "    'prime_genre'\n",
    "]\n",
    "\n",
    "target_col = 'user_rating'\n",
    "\n",
    "cols = num_cols + cat_cols + [target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = data[cols]\n",
    "# возраст записан не в виде числа, исправим это, вырезав последний символ и скастовав к числу\n",
    "data['cont_rating'] = data['cont_rating'].str.slice(0, -1).astype(int)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим на пропущенные значения\n",
    "data.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим на распределение категориальных фичей\n",
    "for col in cat_cols:\n",
    "    print(f\"{col} DISTRIBUTION\")\n",
    "    print(data[col].value_counts())\n",
    "    print('-' * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# как мы видим, в колонке currency только одно значение, можно колонку убрать\n",
    "data = data.drop(columns=['currency'])\n",
    "cat_cols.remove('currency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# посмотрим на распредление величин\n",
    "data.hist(column=num_cols+cat_cols+[target_col], figsize=(14, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь посмотрим на корреляции между фичами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.corr().style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И двойные графики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(data, c=data[target_col], figsize=(15, 15), marker='o', hist_kwds={'bins': 20}, s=10, alpha=.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Упражнение**: Мы только что посмотрели на данные, какой из посмотренных графиков говорит, что мы вряд ли сможем сделать хорошую модель?\n",
    "\n",
    "**Ответ:** ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае фичей мало, поэтому мы легко смогли посмотреть на них. Обычно фичей намного больше и построить такие графики для пар фичей не получится. Тогда в первую очередь можно посмотреть на корреляцию фичей с таргетом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Очистка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные достаточно чистые, в них вряд ли есть какие-то ошибки и не получается с первого взгляда найти выбросы (**outliers**). Поэтому и очищать особо нечего. Но в реальной жизни, ваши данные скорее всего будут полны мусора.\n",
    "\n",
    "Чаще всего нам пришлось бы убирать выбросы, исправлять очевидные ошибки и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание фичей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем сложнее зависимость между фичей и таргетом, тем более сложная модель потребуется, чтобы эту зависимость использовать. Почему бы просто не выбрать семейство самых гибких моделей? Проблема в том, что без большого количества данных для обучения будет происходить **overfit**. Это значит, что модель выучит зависимости, которые случайно появились в обучающих данных из-за ограниченного размера выборки. Такая модель будет хорошо работать на обучающей выборке, но будет плохо справляться с реальной задачей.\n",
    "\n",
    "Используя человеческие знания об устройстве мира, мы можем упростить такую зависимость, создав новые фичи. На самом деле, можно даже не использовать человеческие знания, а просто применить какой-нибудь алгоритм. Например, если у нас есть фичи $x_1, x_2, \\ldots, x_n$, то мы можем добавить новые фичи вида:\n",
    "\n",
    "$$\n",
    "x_{newij} = x_i x_j, i \\ne j\n",
    "$$\n",
    "\n",
    "и понадеяться, что это улучшит предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# добавим категориальную фичу, которая говорит, бесплатное приложение или нет\n",
    "data['is_free'] = (data['price'] == 0)\n",
    "cat_cols.append('is_free')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа с категориальными фичами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большинство алгоритмов не принимает категориальные фичи в чистом виде и нужно из как-то закодировать.\n",
    "\n",
    "Из популярных алгоритмов с категориальными фичами умеет работать **градиентный бустинг**. У этого алгоритма есть много реализаций и среди известных мне с категориальными фичами в виде строк/булевых значений/... умеет работать только `catboost` (библиотека от Яндекса, реализующая градиентный бустинг). Для `xgboost` и `lightgbm` же категориальные фичи нужно превратить в числа. Градиентный бустинг в `sklearn` вообще не умеет по-особенному обрабатывать категориальные фичи и их нужно кодировать как и для других алгоритмов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot-encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой способ закодировать категориальные фичи - **one hot encoding**. Представьте, что у нас есть категориальная фича `prime_genre` с возможными значениями\n",
    "\n",
    "```python\n",
    "['Games', 'Entertainment', 'Education', 'Photo & Video']\n",
    "```\n",
    "\n",
    "Мы можем создать 4 новые бинарные фичи для каждого из столбцов:\n",
    "\n",
    "```python\n",
    "'Entertaiment' -> [0, 1, 0, 0]\n",
    "```\n",
    "\n",
    "В `pandas` очень удобно использовать `get_dummies` для **one-hot-encoding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame.from_dict({'categorical': ['a', 'b', 'a', 'c']})\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте в датафрейм колонки для всех категориальных фичей и обновите список категориальных фичей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols_new = []\n",
    "for col_name in cat_cols:\n",
    "    cat_cols_new.extend(filter(lambda x: x.startswith(col_name), data.columns))\n",
    "cat_cols = cat_cols_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Уменьшение размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто бывает ситуация, когда данные имеют слишком большую размерность, особенно после one-hot-encoding. В таком случае могут помочь алгоритмы для снижения размерности данных. Один из таких алгоритмов - PCA.\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/files.dezyre.com/images/Tutorials/Principal+Component+Analysis.jpg\">\n",
    "Source: https://www.dezyre.com/data-science-in-python-tutorial/principal-component-analysis-tutorial\n",
    "\n",
    "В этом алгоритме мы находим перпендикулярные направления, вдоль которых имеется наибольшая вариация данных (на картинке можно увидеть такие векторы), выбираем из таких направлений $k$ с самой большой вариацией и в качестве новых фичей берем проекции точек на эти направления."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=20)\n",
    "pca.fit(data[num_cols + cat_cols])\n",
    "# Выход pca - numpy матрица, положим ее в новую переменную со всеми фичами\n",
    "X = pca.transform(data[num_cols + cat_cols])\n",
    "# Или есть более простой способ \n",
    "X = pca.fit_transform(data[num_cols + cat_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разделение на train/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того, как мы обучили нашу модель нам нужно как-то понять, насколько она хорошо работает. Выше мы уже говорили про переобучение на данные, с которыми сеть обучалась. Из-за такого переобучения мы не сможем посчитать адекватно узнать точность предсказаний, если проверим точность на тех же данных, на которых обучались. Чтобы с этим бороться обучающую выборку обычно делят на две части train и test. На первой мы будем обучать модель, а на второй проверять, насколько хорошо модель работает. Размер тестовой выборки в 30-40% - неплохой выбор.\n",
    "\n",
    "Иногда данных слишком мало, чтобы жертвовать ими на тестовую часть. Тогда применяется метод, который называет cross validation. Мы посмотрим на то, как он работает в секции про оценку модели.\n",
    "\n",
    "**На самом деле, мы сделали не совсем правильно, потому что разделение на train/test нужно делать до добавления новых фичей/их кодирования итд. Иначе возможны лики из test части в train часть.** Например, при нормализации и PCA мы работаем со всем массивом данных, а значит информация из test попадет и в train. Но для упрощения кода и понимания того, что происходит мы не поделили выборку заранее. Если бы мы все-таки разделили выборку заранее, то нужно использовать fit на train части, а transform уже на обеих."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание: Загуглите как работает эта функция и поделите выборку на две части\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хорошо, теперь можно обучить модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый хороший способ - попробовать максимум разных алгоритмов, посмотреть, какой из них лучше справляется и уже по метрикам выбрать лучший (возможно, объединить предсказания с помощью стэкинга или блендинга, о которых будет на следующем занятии)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наш курс про нейронные сети, поэтому мы не будем фокусироваться на алгоритмах, которые здесь представлены. Все, что нужно знать Ridge - линейная регрессия с регуляризацией. GradientBoosting и RandmForest - алгоритмы, основанные на том, что у нас есть много решающих деревьев, результаты которых потом аггрегируются, чтобы получить финальное предсказание. Как выглядит решающее дерево:\n",
    "<img src=\"https://46gyn61z4i0t1u1pnq2bbk2e-wpengine.netdna-ssl.com/wp-content/uploads/2018/07/what-is-a-decision-tree.png\">\n",
    "Source: https://www.displayr.com/what-is-a-decision-tree/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучше поговорим про метрики. В данном случае у нас задача регрессии, поэтому мы используем две метрики MSE и R_squared. \n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y^i - y_{pred}^i)^2}{\\sum_{i=1}^{n} (y^i - y_{mean})^2}$$\n",
    "\n",
    "Другими словами, это доля объясненной вариации. Максимальное значение = 1, когда у нас есть идеальный предсказатель. Значение меньше 0 означает, что наша модель хуже, чем константая модель, которая выдает просто среднее по таргету."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_preds, y):\n",
    "    print(f'R^2: {r2_score(y_preds, y)}')\n",
    "    print(f'MSE: {mean_squared_error(y_preds, y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем обычную линейную регрессию, минимизирующую сумму квадратов ошибки\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print_metrics(lr.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем линейную регрессию с регуляризацией - в Loss добавляется сумма квадратов коэффициентов, умноженная\n",
    "# на некоторый коэффициент. Это позволяет бороться с тем, что колонки в данных могут быть линейно зависимы\n",
    "# А они у нас почти навреняка линейно зависимы из-за one hot encdoing\n",
    "rlr = Ridge(alpha=1)\n",
    "rlr.fit(X_train, y_train)\n",
    "\n",
    "print_metrics(rlr.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor()\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "print_metrics(gbr.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "print_metrics(rfr.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, методы, основанные на линейной регрессии дали результаты хуже, чем предсказание константы, а вот методы, основанные на лесах уже сработали лучше. \n",
    "\n",
    "**Задание:** поиграйтесь с гиперпараметрами и улучшите предсказания моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До этого мы разбирали случай, когда выбоорка заранее делится на train/test, но часто данных итак не хватает и отдавать их часть на test слишком расточительно. В такой ситуации на помощью приходит кросс валидация:\n",
    "1. Выберем $k$ - количество частей, на которые разобьется наш датасет\n",
    "2. for $ i = 1..k$ \n",
    "    * Обучим модель на всех частях датасета, кроме i-ой.\n",
    "    * Посчитаем метрики или предсказания для i-ой части\n",
    "3. Саггрегируем все все предсказания или усредним метрики\n",
    "\n",
    "Таким образом мы сможем получить более объективные предсказания нашей модели, использовав весь датасет как train и как test, при этом не создав утечек данных.\n",
    "\n",
    "В sklearn существуют уже готовые классы моделей, которые за нас проводят все вышеописанные действия. Но у них есть один минус - выше мы уже писали, что лики могут произойти еще на этапе обработки данных. Избежать этого при ручной разбивке датасета легко, но в случае кросс валидации придется либо сдлеать специальный объект Pipeline, в котором будет скрыта вся обработка данных, и sklearn просто вызовет его $k$ раз, либо руками выбирать индексы объектов с помощью класса KFold и самостоятельно обрабатывать данные. Мы не будем делать ни то, ни другое, но покажем, как это может быть реализовано."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим из кроссвалидации метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(LinearRegression(), X, data[target_col], cv=5, \n",
    "               scoring={'r2_score': make_scorer(r2_score), \n",
    "                        'mean_squared_error': make_scorer(mean_squared_error)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(Ridge(), X, data[target_col], cv=5, \n",
    "               scoring={'r2_score': make_scorer(r2_score, ), \n",
    "                        'mean_squared_error': make_scorer(mean_squared_error)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(GradientBoostingRegressor(), X, data[target_col], cv=5, \n",
    "               scoring={'r2_score': make_scorer(r2_score, ), \n",
    "                        'mean_squared_error': make_scorer(mean_squared_error)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(RandomForestRegressor(), X, data[target_col], cv=5, \n",
    "               scoring={'r2_score': make_scorer(r2_score, ), \n",
    "                        'mean_squared_error': make_scorer(mean_squared_error)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А еще с помощью кросс валидации можно искать гиперпараметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbr_grid_search = GridSearchCV(GradientBoostingRegressor(), \n",
    "                               [{'n_estimators': [100, 150, 440], 'learning_rate': [0.01, 0.05, 0.1, 0.15]}],\n",
    "                               cv=5,\n",
    "                               error_score=make_scorer(mean_squared_error),\n",
    "                               verbose=10)\n",
    "gbr_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gbr_grid_search.best_params_)\n",
    "print(gbr_grid_search.best_score_)\n",
    "print(gbr_grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: попробуйте поиск гиперпараметров для RandomForest, разберитесь как работает KFold по документации sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for train_ind, test_ind in kf.split(X_train):\n",
    "    model.fit(X_train[train_ind], y_train.values[train_ind])\n",
    "    pred = model.predict(X_train[test_ind])\n",
    "    metrics.append(mean_squared_error(y_train.values[test_ind], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[seminar]ml_basics.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
