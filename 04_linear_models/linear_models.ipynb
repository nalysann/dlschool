{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUWCAY5opP87"
   },
   "source": [
    "<p style=\"align: center;\">\n",
    "    <img align=center src=\"../img/dls_logo.jpg\" width=500 height=500>\n",
    "</p>\n",
    "\n",
    "<h1 style=\"text-align: center;\">\n",
    "    Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ\n",
    "</h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wj5MrpmRpP89"
   },
   "source": [
    "<h1 style=\"text-align: center;\">\n",
    "    <b>Градиентный спуск и линейные модели</b>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mTi8pD-2LM3r"
   },
   "source": [
    "В этом ноутбуке мы попробуем реализовать свой градиентный спуск на основе модели линейной регрессии и сравним свою реализацию с библиотечной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Fu3DXZ01RLE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4pKjcf4SQEF"
   },
   "source": [
    "## Построение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ao0ab4nESQEH"
   },
   "source": [
    "Модель нашей линейной регрессии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "APZu5Ra7xn6s"
   },
   "outputs": [],
   "source": [
    "# в этих переменных будут лежать веса, которые мы оценим\n",
    "# w - веса модели, на которые умножаются признаки\n",
    "w = None\n",
    "# b - bias, который добавляется к итоговому результату\n",
    "b = None\n",
    "\n",
    "\n",
    "def mse(preds, y):\n",
    "    \"\"\"\n",
    "    Возвращает среднеквадратичную ошибку между preds и y.\n",
    "    \"\"\"\n",
    "    \n",
    "    return ((preds - y)**2).mean()\n",
    "\n",
    "\n",
    "def solve_weights(X, y):\n",
    "    \"\"\"\n",
    "    Находит параметры w, b по методу наименьших квадратов для X и y.\n",
    "    Решает систему линейных уравнений, к которым приводит метод наименьших \n",
    "    квадратов, для признаков X и значений y.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ключевое слово global позволяет нам использовать\n",
    "    # глобальные переменные, определенные в начале ячейки\n",
    "    global w, b\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    # добавляем к признакам фиктивную размерность, чтобы было удобнее находить bias\n",
    "    bias = np.ones((n, 1))\n",
    "    X_b = np.append(bias, X, axis=1)\n",
    "    \n",
    "    # используем формулу из метода наименьших квадратов\n",
    "    # w_full сожержит коэффициенты w и b, так как мы добавили фиктивную размерность к признакам\n",
    "    w_full = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
    "    \n",
    "    # мы разделяем bias, который лежал в начале вектора w_full, и веса модели w\n",
    "    w = w_full[1:]\n",
    "    b = np.array([w_full[0]])\n",
    "    # нам не нужно возвращать w и b, так как они уже лежат в глобальных переменных\n",
    "    \n",
    "\n",
    "def predict(X):\n",
    "    \"\"\"\n",
    "    Предсказывает значения y, используя текущие параметры модели w и b\n",
    "    \"\"\"\n",
    "    \n",
    "    return X @ w + b\n",
    "\n",
    "\n",
    "def grad_descent(X, y, lr, num_iters=100):\n",
    "    \"\"\"\n",
    "    Находит приближённые значения параметров модели, используя градиентный спуск.\n",
    "    Функция потерь (ошибки) для данной реализации спуска - MSE.\n",
    "    Возвращаемое значение - список значений функции потерь на каждом шаге.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ключевое слово global позволяет нам использовать\n",
    "    # глобальные переменные, определенные в начале ячейки\n",
    "    global w, b\n",
    "    w = np.random.rand(X.shape[1])\n",
    "    b = np.array(np.random.rand(1))\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    \n",
    "    for cur_iter in range(num_iters):\n",
    "        preds = predict(X)\n",
    "        losses.append(mse(preds, y))\n",
    "        \n",
    "        w_grad = np.zeros_like(w)\n",
    "        b_grad = 0\n",
    "        \n",
    "        for sample, prediction, label in zip(X, preds, y):\n",
    "            w_grad += 2 * (prediction - label) * sample\n",
    "            b_grad += 2 * (prediction - label)\n",
    "            \n",
    "        w -= lr * w_grad\n",
    "        b -= lr * b_grad\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xWtYzIacBLwi"
   },
   "source": [
    "Подробнее рассмотрим формулы, которые используются в градиентном спуске. Наша функция потерь:\n",
    "\n",
    "$$\n",
    "L(\\hat{y}) = \\sum_{i = 1}^{n}( \\hat{y}_{i} - y_{i} )^{2}\n",
    "$$\n",
    "\n",
    "Найдём производную:\n",
    "\n",
    "$$\n",
    "\\frac{dL(\\hat{y})}{d\\hat{y}} = \\sum_{i = 1}^{n}2(\\hat{y}_{i} - y_{i})\n",
    "$$\n",
    "\n",
    "где $\\hat{y}$ - это вектор предсказаний, а $y$ - вектор значений.\n",
    "\n",
    "Если у нас есть только два признака, то по определению нашей модели:\n",
    "\n",
    "$$\n",
    "\\hat{y}_{i} = w_1 \\cdot x_{i1} + w_2 \\cdot x_{i2} + b\n",
    "$$\n",
    "\n",
    "Подставим в формулу для функции потерь и возьмём производную:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(\\hat{y})}{ \\partial w_1} = \\sum_{i = 1}^{n} \\frac{\\partial (( \\hat{y}_{i} - y_{i} )^{2})}{\\partial \\hat{y_i}} \\cdot \\frac{\\partial \\hat{y_i}}{\\partial w_1}  =  \n",
    "\\sum_{i = 1}^{n} 2 (\\hat{y_i} - y) \\cdot x_{i1}\n",
    "$$\n",
    "\n",
    "В формуле есть суммирование по всем строчкам $X$ ($x_i$ это $i$-ая строчка $X$, в которой хранятся признаки для $i$-го наблюдения), в коде ему соответствует внешний цикл, итерирующийся по всем наблюдениям. Внутренний цикл нужен для получения производных по всем весам $w_i$, которых в общем случае может быть произвольное количество.\n",
    "\n",
    "В итоге выполнения кода:\n",
    "\n",
    "$$\n",
    "w\\_grad = (\\frac{\\partial L(\\hat{y})}{\\partial w_1} , \\frac{\\partial L(\\hat{y})}{\\partial w_2}, \\frac{\\partial L(\\hat{y})}{\\partial w_3}, \\ldots) = \\nabla L\n",
    "$$ \n",
    "\n",
    "Для обновления весов мы вычитаем градиент, передвигаясь в направлении скорейшего убывания функции:\n",
    "\n",
    "$$\n",
    "w = w - lr \\cdot \\nabla L\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bxys5OMESQEN"
   },
   "source": [
    "## Получение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GvGNqv-QhFCq"
   },
   "outputs": [],
   "source": [
    "def generate_data(range_, a, b, std, num_points=100):\n",
    "    \"\"\"Генерирует данные в заданном промежутке, которые подчиняются\n",
    "    зависимости y = a * x + b + е, где е - нормально распределена со\n",
    "    стандартным отклонением std и нулевым средним.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train = np.random.random(num_points) * (range_[1] - range_[0]) + range_[0]\n",
    "    y_train = a * X_train + b + np.random.normal(0, std, size=X_train.shape)\n",
    "    \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 966,
     "status": "ok",
     "timestamp": 1539364049842,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -300
    },
    "id": "kfjNWvxPSQEO",
    "outputId": "180869e0-ae28-40c8-858e-f766937e6efe"
   },
   "outputs": [],
   "source": [
    "# зададим параметры для искусственных данных\n",
    "real_a = 0.34\n",
    "real_b = 13.7\n",
    "real_std = 7\n",
    "\n",
    "# генерируем данные в промежутке от 0 до 150 с параметрами, которые мы задали выше\n",
    "X_train, y_train = generate_data([0, 150], real_a, real_b, real_std)\n",
    "\n",
    "# просто выведем табличку с данными\n",
    "pd.DataFrame({'X': X_train, 'Y': y_train}).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2125,
     "status": "ok",
     "timestamp": 1539364052157,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -300
    },
    "id": "uyjHOvSuSQEY",
    "outputId": "ad787647-3b81-43ff-f03d-fa500dbdbeaa"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train, c='black')\n",
    "plt.plot(X_train, real_a * X_train + real_b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OSsVqF20SQEj"
   },
   "source": [
    "## Решение с помощью линейной алгебры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rqMulvKhSQEk"
   },
   "outputs": [],
   "source": [
    "# используем функцию, написанную выше, чтобы найти w и b\n",
    "# с помощтю метода наименьших квадратов\n",
    "solve_weights(X_train.reshape(-1, 1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 853,
     "status": "ok",
     "timestamp": 1539364054213,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -300
    },
    "id": "JnVSSMNGSQEo",
    "outputId": "7154fc6c-b5d9-4b7a-ca67-266191ed87c0"
   },
   "outputs": [],
   "source": [
    "# полученные веса лежат в глобальных переменных, выведем их\n",
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NBwYctcZhFDR"
   },
   "source": [
    "Полученные веса очень похожи на те, которые мы задавали при генерации данных. Значит модель получилась хорошей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 780,
     "status": "ok",
     "timestamp": 1539364055884,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -300
    },
    "id": "T8pcThvvSQEt",
    "outputId": "f57c4745-f6d0-4090-8171-dc92cc274609"
   },
   "outputs": [],
   "source": [
    "# выведем данные, истинную зависимость и зависимость,\n",
    "# полученную нами с помощью метода наименьших квадратов\n",
    "plt.scatter(X_train, y_train, c='red')\n",
    "plt.plot(X_train, real_a * X_train + real_b)\n",
    "plt.plot(X_train, X_train.reshape(-1, 1) @ w + b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7MTk_FR2SQEy"
   },
   "source": [
    "## Решение с помощью градиентного спуска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m850g9haSQE1"
   },
   "outputs": [],
   "source": [
    "# найдём параметры с помощью градиентного спуска\n",
    "# чтобы проследить за обучением, мы записываем значение\n",
    "# функции ошибки на каждом шаге и после выводим\n",
    "losses = grad_descent(X_train.reshape(-1, 1), y_train, 1e-9, 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 852,
     "status": "ok",
     "timestamp": 1539364064208,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -300
    },
    "id": "25dgBfNuSQE9",
    "outputId": "b729d5fc-9ef5-4c15-a4d4-41d8ab12faba"
   },
   "outputs": [],
   "source": [
    "# полученные веса лежат в глобальных переменных, выведем их\n",
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9-3mvFTRhFEV"
   },
   "source": [
    "Веса модели получились не похожи, на то, что мы задавали при генерации данных. Модель намного хуже.\n",
    "\n",
    "Стоит отметить, что хуже всего был подобран свободный член $b$, это связано с тем, что данные не нормализованы и параметры $a$ и $b$ имеют очень разные модули, а шаги, которые делает градиентный спуск для обоих параметров одного порядка. Это приводит к тому, что меньший по модулю параметр $a$ быстро подбирается, а параметр почти $b$ перестает изменяться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 914,
     "status": "ok",
     "timestamp": 1539364066361,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -300
    },
    "id": "Hhop71KlSQFB",
    "outputId": "c7c1d664-6ca2-4f6a-a129-e37dc6666268"
   },
   "outputs": [],
   "source": [
    "# выведем график функции потерь \n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "\n",
    "losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1110,
     "status": "ok",
     "timestamp": 1539364068547,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -300
    },
    "id": "ItCyBwfwSQFH",
    "outputId": "58795acc-67a4-4d7e-b0bf-3f3bc5b29fc0"
   },
   "outputs": [],
   "source": [
    "# выведем данные, истинную зависимость и зависимость, полученную нами\n",
    "plt.scatter(X_train, y_train, c='red')\n",
    "plt.plot(X_train, real_a * X_train + real_b)\n",
    "plt.plot(X_train, X_train.reshape(-1, 1) @ w + b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "edEx8RuJhFEw"
   },
   "source": [
    "Градиентный спуск восстановил зависимость хуже, чем метод наименьших квадратов, это вызвано тем, что:\n",
    "\n",
    "* данные **не нормализованы** (подробнее о нормализации в домашнем ноутбуке)\n",
    "\n",
    "* в **методе наименьших квадратов** мы получали решение **аналитически**, поэтому оно гарантировано является наилучшим, в то время как градиентный спуск находит решение лишь приближённо\n",
    "\n",
    "Возникает вопрос, зачем использовать **градиентный спуск**, если он хуже **аналитических** мтеодов? Дело в том, что оптимизация большого количества весов в **нейронных сетях** сликшом сложная задача, которая не может быть решена **аналитически**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e6VOEaY2SQFO"
   },
   "source": [
    "## Данные посложнее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7rNOb6iuJLB-"
   },
   "source": [
    "Загрузим с помощью `pandas` реальные данные и попробуем найти параметры зависимости с помощью метода наименьших квадратов и градиентного спуска, как и в предыдущем примере (так как наш код универсален, нам просто нужно просто вызвать те же функции)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cG4n12xvSQFP"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "uGCTic-qSQFV",
    "outputId": "32fd5980-fdf6-4bb6-f676-bc35d5c0af2d"
   },
   "outputs": [],
   "source": [
    "# так как данные многомерные, мы не можем построить график, как в предыдущем примере, \n",
    "# чтобы увидеть зависимость глазами, поэтому мы просто выведем первые строки таблицы\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2VdsqMNFSQFb"
   },
   "outputs": [],
   "source": [
    "# разделим данные на признаки и значения\n",
    "data, label = np.array(df)[:, 1:5], np.array(df)[:, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "knOBphGnSQFi"
   },
   "source": [
    "### Решение с помощью линейной алгебры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eV3TJ92fSQFj"
   },
   "outputs": [],
   "source": [
    "# используем функцию, написанную выше, чтобы найти\n",
    "# w и b с помощью метода наименьших квадратов\n",
    "solve_weights(data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Yfhm4fISQFm",
    "outputId": "9e9e5af2-7eba-46c4-8959-dd2fef9adae5"
   },
   "outputs": [],
   "source": [
    "# полученные веса лежат в глобальных переменных, выведем их\n",
    "w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KVag7u66SQFq",
    "outputId": "2a6398d5-ad59-423d-8428-6123aeeb202f"
   },
   "outputs": [],
   "source": [
    "# выведем значение функции ошибки, чтобы позже сравнить с результатом градиентного спуска\n",
    "mse(predict(data), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gJqoPz4RSQFt"
   },
   "source": [
    "### Решение с помощью градиентного спуска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cFXllJyQSQFv"
   },
   "outputs": [],
   "source": [
    "# найдём параметры с помощью градиентного спуска\n",
    "# чтобы проследить за обучением, мы записываем значение\n",
    "# функции ошибки на каждом шаге и после выводим\n",
    "losses = grad_descent(data, label, 1e-9, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "27q36bXrSQF1",
    "outputId": "f8351eeb-6bb4-44ad-89ed-776d2f2586f4"
   },
   "outputs": [],
   "source": [
    "# полученные веса лежат в глобальных переменных, выведем их\n",
    "w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0MzZJdjQSQF5",
    "outputId": "b1f5031c-c346-4788-c155-eeeacb21cc82"
   },
   "outputs": [],
   "source": [
    "# выведем график функции потерь \n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GqmUW4-lLPq_",
    "outputId": "3c28cdb2-84f1-4ef7-a8a8-49868c0a31ac"
   },
   "outputs": [],
   "source": [
    "# выведем значение функции ошибки\n",
    "mse(predict(data), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ws5k-CQChFFi"
   },
   "source": [
    "Как мы видим, **градиентный спуск** опять нашел значительно более плохое решение. Если нормализовать данные, то **градиентный спуск** будет сходиться лучше и разница будет не такой заметной. \n",
    "\n",
    "В домашнем задании вы научитесь нормализовывать данные. После этого вы можете вернуться в этот ноутбук и запустить градиентный спуск, предварительно использовав нормализацию."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[seminar]linear_models.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
