{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUWCAY5opP87"
   },
   "source": [
    "<p style=\"align: center;\">\n",
    "    <img align=center src=\"../img/dls_logo.jpg\" width=500 height=500>\n",
    "</p>\n",
    "\n",
    "<h1 style=\"text-align: center;\">\n",
    "    <b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b>\n",
    "</h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wj5MrpmRpP89"
   },
   "source": [
    "<h1 style=\"text-align: center;\">\n",
    "    Производная и градиент\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O1xIGzO5pP8-"
   },
   "source": [
    "**На основе https://github.com/romasoletskyi/Machine-Learning-Course**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODaZDX75pP8-"
   },
   "source": [
    "## Приращение линейной функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tldAx431pP8_"
   },
   "source": [
    "Давайте рассмотрим линейную функцию $y=kx+b$ и построим её график: \n",
    "\n",
    "<img src=\"../img/linear_models_derivative_8.svg\"> \n",
    "\n",
    "Введём понятие **приращения** функции в точке $(x, y)$ как отношение вертикального изменения (измненеия функции по вертикали) $\\Delta y$ к горизонтальному изменению $\\Delta x$ и вычислим приращение для линейной функции:\n",
    "\n",
    "$$\n",
    "slope=\\frac{\\Delta y}{\\Delta x}=\\frac{y_2-y_1}{x_2-x_1}=\\frac{kx_2+b-kx_1-b}{x_2-x_1}=k\\frac{x_2-x_1}{x_2-x_1}=k\n",
    "$$  \n",
    "\n",
    "Видим, что приращение в точке у прямой не зависит от $x$ и $\\Delta x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObcFoC9JpP9A"
   },
   "source": [
    "## Приращение произвольной функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNfWm09WpP9A"
   },
   "source": [
    "Но что, если функция не линейная, а произвольная? В таком случае просто нарисуем **касательную** в точке, в которой ищем приращение, и будем смотреть уже на приращение касательной. Так как касательная - это прямая, мы уже знаем, какое у неё приращение (см. выше).\n",
    "\n",
    "<img src=\"../img/linear_models_derivative_9.svg\" width=500> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "137KHuBjpP9B"
   },
   "source": [
    "Имея график функции, мы, конечно, можем нарисовать касательную в точке. Но часто функции заданы аналитически, и хочется уметь сразу быстро получать формулу для приращения функциии в точке. Тут на помощь приходит **производная**. Давайте посмотрим на определение производной и сравним его с нашим понятием приращения:\n",
    "\n",
    "$$\n",
    "f'(x) = \\lim_{\\Delta x \\to 0}\\frac{\\Delta y}{\\Delta x} = \\lim_{\\Delta x \\to 0}\\frac{f(x + \\Delta x) - f(x)}{\\Delta x}\n",
    "$$  \n",
    "\n",
    "То есть, по сути, значение производной функции в точке - это и есть приращение функции, если мы устремляем длину отрезка $\\Delta x$ к нулю."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7YksIkmlpP9C"
   },
   "source": [
    "Посомтрим на интерактивное демо, демонстрирующее стремление $\\Delta x$ к нулю:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4DbxljwpP9F"
   },
   "outputs": [],
   "source": [
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 764,
     "status": "error",
     "timestamp": 1540841307892,
     "user": {
      "displayName": "Илья Дмитриевич Захаркин",
      "photoUrl": "",
      "userId": "09157257912804633784"
     },
     "user_tz": -180
    },
    "id": "v9rhGojJpP9D",
    "outputId": "ba6596ca-53ac-45aa-ea53-affe48ac21ac"
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJ_xbrHXpP9I"
   },
   "outputs": [],
   "source": [
    "@interact(scale=(-0.5, 4.0, 0.1))\n",
    "def f(scale=1.0):\n",
    "    z = 10**scale\n",
    "    x_min = 1.5 - 6 / z\n",
    "    x_max = 1.5 + 6 / z\n",
    "    l_min = 1.5 - 4 / z\n",
    "    l_max = 1.5 + 4 / z\n",
    "    xstep = (x_max - x_min) / 100\n",
    "    lstep = (l_max - l_min) / 100\n",
    "    \n",
    "    x = np.arange(x_min, x_max, xstep)\n",
    "    \n",
    "    plt.plot(x, np.sin(x), '-b')     \n",
    "    \n",
    "    plt.plot((l_min,l_max), (np.sin(l_min), np.sin(l_max)), '-r')\n",
    "    plt.plot((l_min,l_max), (np.sin(l_min), np.sin(l_min)), '-r')\n",
    "    plt.plot((l_max,l_max), (np.sin(l_min), np.sin(l_max)), '-r')\n",
    "    \n",
    "    yax = plt.ylim()    \n",
    "    \n",
    "    plt.text(l_max + 0.1 / z, (np.sin(l_min) + np.sin(l_max)) / 2, \"$\\Delta y$\")\n",
    "    plt.text((l_min + l_max) / 2, np.sin(l_min) - (yax[1] - yax[0]) / 20, \"$\\Delta x$\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print('slope =', (np.sin(l_max) - np.sin(l_min)) / (l_max - l_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8CYa2CRpP9N"
   },
   "source": [
    "Видим, что при уменьшении отрезка $\\Delta x$, значение приращения стабилизируется (перестаёт изменяться). Это число и есть приращение функции в точке, равное производной функции в точке. Производную функции $f(x)$ в точке x обознают как $f'(x)$ или как $\\frac{d}{dx}f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMwBqnhVpP9N"
   },
   "source": [
    "### Пример вычисления проиводной"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6R_rnMsqpP9P"
   },
   "source": [
    "Возьмём производную по определению:\n",
    "\n",
    "1. $f(x)=x$  \n",
    "\n",
    "$$\n",
    "\\frac{\\Delta y}{\\Delta x}=\\frac{x+\\Delta x-x}{\\Delta x}=1\\Rightarrow \\frac{d}{dx}(x)=1\n",
    "$$  \n",
    "\n",
    "2. $f(x)=x^2$  \n",
    "\n",
    "$$\n",
    "\\frac{\\Delta y}{\\Delta x}=\\frac{(x+\\Delta x)^2-x^2}{\\Delta x}=\\frac{x^2+2x\\Delta x+\\Delta x^2-x^2}{\\Delta x}=2x+\\Delta x\\rightarrow 2x (\\Delta x\\rightarrow 0)\\Rightarrow \\frac{d}{dx}(x^2)=2x\n",
    "$$  \n",
    "    \n",
    "3. В общем случае для степенной функции $f(x)=x^n$ формула будет такой:  \n",
    "\n",
    "$$\n",
    "\\frac{d}{dx}(x^n)=nx^{n-1}\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KP4jUOaqpP9P"
   },
   "source": [
    "## Правила вычисления производной"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8fb0go1lpP9Q"
   },
   "source": [
    "Рассмотрим правила дифференцирования:  \n",
    "\n",
    "1. Если $f(x)$ - константа, то её производная (приращение) равно $0$:  \n",
    "\n",
    "    $$\n",
    "    (C)' = 0\n",
    "    $$\n",
    "\n",
    "2. Производная суммы функций - это сумма производных:  \n",
    "\n",
    "    $$\n",
    "    (f(x) + g(x))' = f'(x) + g'(x)\n",
    "    $$\n",
    "\n",
    "3. Производная разности - это разность производных:\n",
    "\n",
    "    $$\n",
    "    (f(x) - g(x))' = f'(x) - g'(x)\n",
    "    $$\n",
    "\n",
    "4. Производная произведения функций:  \n",
    "\n",
    "    $$\n",
    "    (f(x)g(x))' = f'(x)g(x) + f(x)g'(x)\n",
    "    $$\n",
    "\n",
    "5. Производная частного функций: \n",
    "\n",
    "    $$\n",
    "    \\left(\\frac{f(x)}{g(x)}\\right)'=\\frac{f'(x)g(x)-g'(x)f(x)}{g^2(x)}\n",
    "    $$\n",
    "\n",
    "6. Производная сложной функции (**chain rule**):  \n",
    "\n",
    "    $$\n",
    "    (f(g(x)))'=f'(g(x))g'(x)\n",
    "    $$\n",
    "\n",
    "    Можно записать ещё так: \n",
    "\n",
    "    $$\n",
    "    \\frac{d}{dx}(f(g(x)))=\\frac{df}{dg}\\frac{dg}{dx}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZFTReW5ppP9R"
   },
   "source": [
    "### Примеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "grjqr2h4pP9R"
   },
   "source": [
    "1. Вычислим производную функции $f(x) = \\frac{x^2}{cos(x)} + 100$:  \n",
    "\n",
    "$$\n",
    "f'(x) = \\left(\\frac{x^2}{cos(x)}+100\\right)' = \\left(\\frac{x^2}{cos(x)}\\right)' + (100)' = \\frac{(2x)\\cos(x) - x^2(-\\sin(x))}{cos^2(x)}\n",
    "$$\n",
    "\n",
    "\n",
    "2. Вычислим производную функции $f(x) = tg(x)$:  \n",
    "\n",
    "$$\n",
    "f'(x) = \\left(tg(x)\\right)' = \\left(\\frac{\\sin(x)}{\\cos(x)}\\right)' = \\frac{\\cos(x)\\cos(x) - \\sin(x)(-\\sin(x))}{cos^2(x)} = \\frac{1}{cos^2(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXP_YETzpP9T"
   },
   "source": [
    "## Частные производные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJDwZBCZpP9T"
   },
   "source": [
    "Когда мы имеем функцию многих переменных, её уже сложнее представить себе в виде рисунка (в случае более 3-х переменных это действительно не всем дано). Однако формальные правила взятия производной у таких функций сохраняются. Они в точности совпадают с теми, которые рассмотрены выше для функции одной переменной.\n",
    "\n",
    "Итак, правило взятия частной производной функции многих переменных:\n",
    "\n",
    "1. Пусть $f(\\bar{x}) = f(x_1, x_2, \\ldots, x_n)$ - функция многих переменных.\n",
    "\n",
    "\n",
    "2. Частная проиводная по $x_i$ этой функции - это производная по $x_i$, считая все остальные переменные **константами**.\n",
    "\n",
    "Более математично, частная производная функции $f(x_1,x_2,\\ldots,x_n)$ по $x_i$ равна:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(x_1,x_2,\\ldots,x_n)}{\\partial x_i}=\\frac{df_{x_1,\\ldots,x_{i-1},x_{i+1},\\ldots,x_n}(x_i)}{dx_i},\n",
    "$$  \n",
    "\n",
    "где $f_{x_1,\\ldots,x_{i-1},x_{i+1},\\ldots,x_n}(x_i)$ означает, что переменные $x_1,\\ldots,x_{i-1},x_{i+1},\\ldots,x_n$ - это фиксированные значения, и с ними нужно обращаться как с константами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3uPIkZ-wpP9U"
   },
   "source": [
    "### Примеры   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aodpt9VppP9V"
   },
   "source": [
    "1. Найдём частные производные функции $f(x, y) = -x^7 + (y - 2)^2 + 140$ по $x$ и по $y$:\n",
    "\n",
    "$$\n",
    "f_x'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{x}} = -7x^6\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_y'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{y}} = 2(y - 2)\n",
    "$$\n",
    "\n",
    "2. Найдём частные производные функции $f(x, y, z) = \\sin(x)\\cos(y)tg(z)$ по $x$, по $y$ и по $z$:\n",
    "\n",
    "$$\n",
    "f_x'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{x}} = \\cos(x)\\cos(y)tg(z)\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_y'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{y}} = \\sin(x)(-\\sin(y))tg(z)\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_z'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{y}} = \\frac{\\sin(x)\\cos(y)}{\\cos^2{z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nrmPlYyrpP9X"
   },
   "source": [
    "## Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeD8U4CApP9X"
   },
   "source": [
    "**Градиентом** функции $f(\\bar{x})$, где $\\bar{x} \\in \\mathbb{R^n}$, то есть $\\bar{x} = (x_1, x_2, .., x_n)$, называется вектор из частных производных функции $f(\\bar{x})$:\n",
    "\n",
    "$$\n",
    "grad(f) = \\nabla f(\\bar{x}) = \\left(\\frac{\\partial{f(\\bar{x})}}{\\partial{x_1}}, \\frac{\\partial{f(\\bar{x})}}{\\partial{x_2}}, \\ldots, \\frac{\\partial{f(\\bar{x})}}{\\partial{x_n}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DcjYCHOepP9Y"
   },
   "source": [
    "Пусть задана функция $f(x)$. Хотим найти аргумент, при котором она достигает минимума.\n",
    "\n",
    "Алгоритм градиентного спуска:\n",
    "\n",
    "1. $x^0$ - начальное значение (обычно берётся просто из разумных соображений или случайное).\n",
    "\n",
    "2. $x^i = x^{i-1} - \\alpha \\nabla f(x^{i-1})$, где $\\nabla f(x^{i-1})$ - это градиент функции $f$, в который подставлено значение $x^{i-1}$.\n",
    "\n",
    "3. Выполнять пункт 2, пока не выполнится условие остановки: $||x^{i} - x^{i-1}|| < eps$, где $||x^{i} - x^{i-1}|| = \\sqrt{(x_1^i - x_1^{i-1})^2 + .. + (x_n^i - x_n^{i-1})^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zX1miuQ0pP9Z"
   },
   "source": [
    "### Примеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t1M6agxdpP9Z"
   },
   "source": [
    "Посчитаем формулу градиентного спуска для функции $f(x) = 10x^2$:\n",
    "\n",
    "$$\n",
    "x^i = x^{i-1} - \\alpha \\nabla f(x^{i-1}) = x^{i-1} - \\alpha f'(x^{i-1}) = x^{i-1} - \\alpha (20x^{i-1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EqjopRZVpP9b"
   },
   "source": [
    "Имея эту формулу, напишем код градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evLahkyIpP9c"
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 10 * x**2\n",
    "\n",
    "def gradient_descent(alpha=0.001, eps=0.01):\n",
    "    # начальная инициализация\n",
    "    x_pred = 100\n",
    "    x = 50\n",
    "    \n",
    "    for i in tqdm(range(100000)):\n",
    "        # смотрим, на каком мы шаге\n",
    "        # print(i)\n",
    "        \n",
    "        # условие остановки\n",
    "        if np.sum((x - x_pred)**2) < eps**2:\n",
    "            break\n",
    "            \n",
    "        x_pred = x\n",
    "        # по формуле выше\n",
    "        x = x_pred - 20 * alpha * x_pred\n",
    "        \n",
    "        sleep(0.005)\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 501,
     "status": "ok",
     "timestamp": 1539764935297,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -180
    },
    "id": "9k8A7ei8pP9g",
    "outputId": "7d0401a9-5810-4215-89af-4a9d077c05eb"
   },
   "outputs": [],
   "source": [
    "x_min = gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1539764937543,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -180
    },
    "id": "uGOVAybRpP9k",
    "outputId": "2c6d1f12-ce1e-491f-ca98-6e1fe98849c7"
   },
   "outputs": [],
   "source": [
    "x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 598,
     "status": "ok",
     "timestamp": 1539764938543,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -180
    },
    "id": "_EW8AY8VpP9n",
    "outputId": "4d7b79da-706d-4d04-c1ba-29d39625760f"
   },
   "outputs": [],
   "source": [
    "f(x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tsqfsTezpP9q"
   },
   "source": [
    "Посчитаем формулу градиентного спуска для функции $f(x, y) = 10x^2 + y^2$:\n",
    "\n",
    "$$\n",
    "\\left(\\begin{matrix} x^i \\\\ y^i \\end{matrix}\\right) = \\left(\\begin{matrix} x^{i-1} \\\\ y^{i-1} \\end{matrix}\\right) - \\alpha \\nabla f(x^{i-1}, y^{i-1}) = \\left(\\begin{matrix} x^{i-1} \\\\ y^{i-1} \\end{matrix}\\right) - \\alpha \\left(\\begin{matrix} \\frac{\\partial{f(x^{i-1}, y^{i-1})}}{\\partial{x}} \\\\ \\frac{\\partial{f(x^{i-1}, y^{i-1})}}{\\partial{y}} \\end{matrix}\\right) = x^{i-1} - \\alpha \\left(\\begin{matrix} 20x^{i-1} \\\\ 2y^{i-1} \\end{matrix}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sBnijsKLpP9r"
   },
   "source": [
    "Осталось написать код, выполняющий градиентный спуск, пока не выполнится условие остановки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_rDsja-pP9s"
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 10 * x[0]**2 + x[1]**2\n",
    "\n",
    "def gradient_descent(alpha=0.01, eps=0.001):\n",
    "    # начальная инициализация\n",
    "    x_prev = np.array([100, 100])\n",
    "    x = np.array([50, 50])\n",
    "    \n",
    "    for i in tqdm(range(100000)):\n",
    "        # смотрим, на каком мы шаге\n",
    "        # print(_)\n",
    "        \n",
    "        # условие остановки\n",
    "        if np.sum((x - x_prev)**2) < eps**2:\n",
    "            break\n",
    "            \n",
    "        x_prev = x\n",
    "        # по формуле выше\n",
    "        x = x_prev - alpha * np.array(20 * x_prev[0], 2 * x_prev[1])\n",
    "        \n",
    "        sleep(0.005)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "boueQCnXpP9u"
   },
   "outputs": [],
   "source": [
    "x_min = gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6pyhQsmXpP9x"
   },
   "outputs": [],
   "source": [
    "x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ytAfn_X7pP90"
   },
   "outputs": [],
   "source": [
    "f(x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YyKaCWuJpP93"
   },
   "source": [
    "## Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jrjiC9mUpP93"
   },
   "source": [
    "1. Вычислите производную функции $f(x)=\\frac{1}{x}$ по определению и сравните с производной степенной функции в общем случае.\n",
    "\n",
    "2. Найдите производную функции $Cf(x)$, где С - число.\n",
    "\n",
    "3. Найдите производные функций:\n",
    "\n",
    "$$\n",
    "f(x)=x^3+3\\sqrt{x}-e^x\n",
    "$$\n",
    "    \n",
    "$$\n",
    "f(x)=\\frac{x^2-1}{x^2+1}\n",
    "$$\n",
    "    \n",
    "$$\n",
    "\\sigma(x)=\\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "    \n",
    "$$\n",
    "L(y, \\hat{y}) = (y-\\hat{y})^2\n",
    "$$\n",
    "\n",
    "4. Напишите формулу и код для градиентного спуска для функции:\n",
    "\n",
    "    $$\n",
    "    f(w, x) = \\frac{1}{1 + e^{-wx}}\n",
    "    $$\n",
    "\n",
    "    То есть по аналогии с примером 2 вычислите частные производные по $w$ и по $x$ и запишите формулу в векторном виде.\n",
    "\n",
    "В задаче 3 производную нужно брать по $\\hat{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxDBOB04pP93"
   },
   "source": [
    "## Полезные ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nm0VC825pP95"
   },
   "source": [
    "#### Функции\n",
    "\n",
    "* Прикольный сайт с рисунками, где кривые задаются уравнениями и функциями: https://www.desmos.com/calculator/jwshvscdzb\n",
    "\n",
    "#### Производные\n",
    "\n",
    "* Про то, как брать частные производные: http://www.mathprofi.ru/chastnye_proizvodnye_primery.html\n",
    "\n",
    "* Сайт на английском, но там много видеоуроков и задач по производным: https://www.khanacademy.org/math/differential-calculus/derivative-intro-dc\n",
    "\n",
    "* Задачи на частные производные: http://ru.solverbook.com/primery-reshenij/primery-resheniya-chastnyx-proizvodnyx/  \n",
    "\n",
    "* Ещё задачи на частные производные: https://xn--24-6kcaa2awqnc8dd.xn--p1ai/chastnye-proizvodnye-funkcii.html  \n",
    "\n",
    "* Матричные производные: http://nabatchikov.com/blog/view/matrix_der  \n",
    "\n",
    "#### Градиентный спуск\n",
    "\n",
    "* Основная статья по градиентному спуску: http://www.machinelearning.ru/wiki/index.php?title=Метод_градиентного_спуска\n",
    "\n",
    "* Статья на Хабре про градиетный спуск для нейронных сетей: https://habr.com/post/307312/  \n",
    "\n",
    "#### Методы оптимизации в нейронных сетях\n",
    "\n",
    "* Сайт с анимациями того, как сходятся алгоритмы градиентного спуска: www.denizyuret.com/2015/03/alec-radfords-animations-for.html\n",
    "\n",
    "* Статья на Хабре про методы оптимизации (градиентный спуск) в нейронных сетях: https://habr.com/post/318970/\n",
    "\n",
    "* Ещё один сайт (на английском) про методы оптимизации (градиентный спуск) в нейронных сетях (очень подробно): http://ruder.io/optimizing-gradient-descent/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[seminar]derivative_gradient.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
