{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EvkV_R3vzl_m"
   },
   "source": [
    "<p style=\"align: center;\">\n",
    "    <img align=center src=\"../img/dls_logo.jpg\" width=500 height=500>\n",
    "</p>\n",
    "\n",
    "<h1 style=\"text-align: center;\">\n",
    "    <b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b>\n",
    "</h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4mYrykelzl_o"
   },
   "source": [
    "<h1 style=\"text-align: center;\">\n",
    "    Домашнее задание: градиентный спуск и линейные модели\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1xoftyezl_p"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from matplotlib import pylab, gridspec, pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yY2kUgovzl_s"
   },
   "source": [
    "## Градиентный спуск: повторение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ctYan0Tfzl_t"
   },
   "source": [
    "Рассмотрим функцию от двух переменных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xDCA6t7Azl_v"
   },
   "outputs": [],
   "source": [
    "def f(x1, x2):\n",
    "    return np.sin(x1) ** 2 + np.sin(x2) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uV1aB-CQzl_w"
   },
   "source": [
    "**Напоминание.**\n",
    "\n",
    "Что мы хотим? Мы хотим найти минимум этой функции (в машинном обучении мы обычно хотим найти минимум **функции потерь**, например, **MSE**), а точнее найти $x_1$ и $x_2$ такие, что при них значение $f(x_1,x_2)$ минимально, то есть **точку экстремума**. \n",
    "\n",
    "Как мы будем искать эту точку? Используем методы оптимизации (в нашем случае - минимизации). Одним из таких методов и является **градиентный спуск**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7b185-dEzl_x"
   },
   "source": [
    "Реализуем функцию, которая будет осуществлять градиентный спуск для функции $f$.\n",
    "\n",
    "**Примечание:** вам нужно посчитать частные производные именно **аналитически** и **переписать их в код**, а не считать производные численно (через отношение приращения функции к приращению аргумента) - в этих двух случаях могут различаться ответы, поэтому будьте внимательны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VfgA69Vizl_y"
   },
   "outputs": [],
   "source": [
    "def grad_descent(lr, num_iter=100):\n",
    "    \"\"\"\n",
    "    Функция, которая реализует градиентный спуск в минимум для функции f от двух переменных. \n",
    "        param lr - learning rate алгоритма\n",
    "        param num_iter - количество итераций градиентного спуска\n",
    "    \"\"\"\n",
    "    \n",
    "    global f\n",
    "    # в начале градиентного спуска инициализируем значения x1 и x2 какими-нибудь числами\n",
    "    cur_x1, cur_x2 = 1.5, -1\n",
    "    # будем сохранять значения аргументов и значений функции в процессе градиентного спуска в переменную steps\n",
    "    steps = []\n",
    "    \n",
    "    # итерация цикла - шаг градиентнго спуска\n",
    "    for iter_num in range(num_iter):\n",
    "        steps.append([cur_x1, cur_x2, f(cur_x1, cur_x2)])\n",
    "        \n",
    "        # чтобы обновить значения cur_x1 и cur_x2, как мы помним с последнего занятия, \n",
    "        # нужно найти производные (градиенты) функции f по этим переменным\n",
    "        grad_x1 = np.sin(2 * cur_x1)\n",
    "        grad_x2 = np.sin(2 * cur_x2)\n",
    "                 \n",
    "        # после того, как посчитаны производные, можно обновить веса\n",
    "        # не забудьте про lr\n",
    "        cur_x1 -= lr * grad_x1\n",
    "        cur_x2 -= lr * grad_x2\n",
    "\n",
    "    return np.array(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JKffGXNrzl_0"
   },
   "source": [
    "Запустим градиентный спуск:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XVtijv6Dzl_1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "steps = grad_descent(lr=0.5, num_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKaPajd6zl_4"
   },
   "source": [
    "Визуализируем точки градиентного спуска на 3D-графике нашей функции. Кружочками будут обозначены точки (тройки $x_1, x_2, f(x_1, x_2)$), по которым ваш алгоритм градиентного спуска двигался к минимуму.\n",
    "\n",
    "Для того, чтобы нарисовать этот график, мы и сохраняли значения $cur\\_x_1, cur\\_x_2, f(cur\\_x_1, cur\\_x_2)$ в `steps` в процессе спуска.\n",
    "\n",
    "Если у вас правильно написана функция `grad_descent_2d`, то кружочки на картинке должны сходиться к одной из точек минимума функции. Вы можете менять начальные приближения алгоритма, значения `lr` и `num_iter` и получать разные результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y2PqLNfEzl_5"
   },
   "outputs": [],
   "source": [
    "# %matplotlib osx\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "path = []\n",
    "\n",
    "X, Y = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "# редактируем настройки отображения траектории, сделанной градиентым спуском\n",
    "ax.plot(xs=steps[:, 0], ys=steps[:, 1], zs=steps[:, 2],\n",
    "        marker='o', markersize=8, zorder=3, \n",
    "        markerfacecolor='red', lw=3, c='green')\n",
    "\n",
    "ax.plot_surface(X, Y, f(X, Y), cmap=cm.coolwarm)\n",
    "ax.set_zlim(0, 5)\n",
    "ax.view_init(elev=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vvgaD86KzmAC"
   },
   "source": [
    "## Линейные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQ50iMzwzmAC"
   },
   "source": [
    "Возьмём код для линейной регрессии с семинара. Напомним, что найти веса $W$ и $b$ для линейной регрессии можно двумя способами: обращением матриц (функция `solve_weights`) и градиентным спуском (функция `grad_descent`). Мы здесь будем рассматривать градиентный спуск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Mm7F1rizmAD"
   },
   "outputs": [],
   "source": [
    "W = None\n",
    "b = None\n",
    "\n",
    "\n",
    "def mse(y_pred, y):\n",
    "    return np.mean((y_pred - y) ** 2)\n",
    "\n",
    "\n",
    "def grad_descent(X, y, lr, num_iter=100):\n",
    "    global W, b\n",
    "    \n",
    "    np.random.seed(40)\n",
    "    \n",
    "    W = np.random.rand(X.shape[1])\n",
    "    b = np.array(np.random.rand(1))\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    \n",
    "    for iter_num in range(num_iter):\n",
    "        y_pred = predict(X)\n",
    "        losses.append(mse(y_pred, y))\n",
    "        \n",
    "        W_grad = np.zeros_like(W)\n",
    "        b_grad = 0\n",
    "        \n",
    "        for sample, prediction, label in zip(X, y_pred, y):\n",
    "            W_grad += 2 * (prediction - label) * sample\n",
    "            b_grad += 2 * (prediction - label)\n",
    "        \n",
    "        W -= lr * W_grad\n",
    "        b -= lr * b_grad\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "def predict(X):\n",
    "    global W, b\n",
    "    return np.squeeze(X @ W + b.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYKGc0aHzmAG"
   },
   "source": [
    "Рассмотрим функцию:  \n",
    "\n",
    "$$\n",
    "f(x, y) = 0.43x+0.5y + 0.67\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kQXFf_Y9zmAH"
   },
   "source": [
    "**Напоминание.** Что мы хотим? Мы хотим уметь \"восстанавливать функцию\" - то есть предсказывать значения функции в точках с координатами $(x, y)$ (именно так и получается 3D-график - $(x, y, f(x,y))$ в пространстве). В чём сложность? Нам дан только конечный небольшой набор точек ($30$ в данном случае), по которому мы хотим восстановить зависимость, по сути, непрерывную. Линейная регрессия как раз подходит для восстановления линейной зависимости (а у нас функция сейчас как раз линейная - только сложение аргументов и умножение на число)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sYO-jgNszmAK"
   },
   "source": [
    "Cгерерируем шумные данные из этой функции (как на семинаре):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qKjm2cQrzmAK"
   },
   "outputs": [],
   "source": [
    "np.random.seed(40)\n",
    "func = lambda x, y: (0.43 * x + 0.5 * y + 0.67 + np.random.normal(0, 7, size=x.shape))\n",
    "\n",
    "X = np.random.sample(30) * 10\n",
    "Y = np.random.sample(30) * 150\n",
    "\n",
    "result_train = [func(x, y) for x, y in zip(X, Y)]\n",
    "data_train = np.concatenate([X.reshape(-1, 1), Y.reshape(-1, 1)], axis=1)\n",
    "\n",
    "pd.DataFrame({'x': X, 'y': Y, 'res': result_train}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ETB66yWyzmAN"
   },
   "source": [
    "Посмотрим, что же мы сгенерировали:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DUXvT674zmAN"
   },
   "outputs": [],
   "source": [
    "# %matplotlib osx\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "X, Y = np.meshgrid(np.linspace(0, 10, 100), np.linspace(0, 150, 100))\n",
    "ax.scatter(xs=data_train[:, 0], ys=data_train[:, 1], zs=result_train, c='red')\n",
    "ax.plot_surface(X,Y, 0.43*X + 0.5*Y + 0.67, color='green', alpha=0.3)\n",
    "\n",
    "ax.view_init(elev=60)\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "09leW_BazmAQ"
   },
   "source": [
    "Теперь давайте попробуем применить к этим данным нашу линейную регрессию и с помощью неё предсказать истинный график функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XXYMvQxfzmAR"
   },
   "outputs": [],
   "source": [
    "losses = grad_descent(data_train, result_train, 1e-2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UITmWCUNzmAT"
   },
   "outputs": [],
   "source": [
    "W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P8fA0-DczmAV"
   },
   "source": [
    "Посмотрим на график лосса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LPNor1fzmAW"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVYB8qUOzmAZ"
   },
   "source": [
    "И на полученную разделяющую плоскость:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMdpNJM6zmAa"
   },
   "outputs": [],
   "source": [
    "# %matplotlib osx\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "X, Y = np.meshgrid(np.linspace(0, 10, 100), np.linspace(0, 150, 100))\n",
    "ax.scatter(xs=data_train[:, 0], ys=data_train[:, 1], zs=result_train, c='red')\n",
    "ax.plot_surface(X, Y, 0.43 * X + 0.5 * Y + 0.67, color='green', alpha=0.3)\n",
    "ax.plot_surface(X,Y, W[0] * X + W[1] * Y + b, color='blue', alpha=0.3)\n",
    "\n",
    "ax.view_init(elev=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sFczLgyuzmAe"
   },
   "source": [
    "Зелёная плоскость - истинная функция, синяя плоскость - предсказание."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0BzdivVFzmAf"
   },
   "source": [
    "О нет, лосс и коэффициенты ($W$ и $b$) нашей модели быстро уходит в небеса, и график предсказан неправильно. Почему такое происходит?\n",
    "\n",
    "В данном случае дело в том, что признаки имеют разный **масштаб** (посмотрите на значения $X$ и $Y$ - они лежат в разных диапазонах). Многие модели машинного обучения, в том числе линейные, будут плохо работать в таком случае (на самом деле это зависит от метода оптимизации, сейчас это градиентный спуск).  \n",
    "\n",
    "Есть несколько способов **масштабирования**:\n",
    "\n",
    "1. **Нормализация** (она же **стандартизация**, `StandardScaling`):  \n",
    "\n",
    "   $$\n",
    "   x_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}\n",
    "   $$\n",
    "   \n",
    "   где $j$ - номер признака, $i$ - номер объекта.\n",
    "   \n",
    "   То есть вычитаем среднее по столбцу и делим на корень из дисперсии.\n",
    "\n",
    "\n",
    "2. **Приведение к отрезку $[0,1]$** (`MinMaxScaling`):  \n",
    "\n",
    "   $$\n",
    "   x_{ij} = \\frac{x_{ij} - \\min_j}{\\max_j - \\min_j}\n",
    "   $$\n",
    "   \n",
    "   где $j$ - номер признака, $i$ - номер объекта.\n",
    "   \n",
    "   То есть вычитаем минимум по столбцу и делим на разницу между минимумом и максимумом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4gOqi21ezmAh"
   },
   "source": [
    "### Нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XfoQbSEyzmAi"
   },
   "source": [
    "Посмотрим на среднее и разброс значений в признаках (координатах) до масштабирования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npJ9aGzZzmAi"
   },
   "outputs": [],
   "source": [
    "data_train.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SghPwBF3zmAk"
   },
   "outputs": [],
   "source": [
    "data_train.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZcvJRf1zmAm"
   },
   "source": [
    "То есть в первом столбце у нас среднее $5.6$ и среднеквадратичное отклонение $2.7$, а во втором столбце среднее $70.8$ и среднеквадратичное отклонение $45.3$.  \n",
    "\n",
    "Будьте внимательны: среднеквадратичное отклонение **НЕ** говорит о том, каков **максимальный** разброс, оно лишь указывает на числовой интервал, вероятность попадания в который у данного признака высока (то есть, например, в интервал $[2.9, 8.3]$ в первом случае). Для большей информации смотрите [среднеквадратичное отклонение](https://ru.wikipedia.org/wiki/Среднеквадратическое_отклонение) и [доверительные интервалы](https://ru.wikipedia.org/wiki/Доверительный_интервал).\n",
    "\n",
    "Однако факт в том, что масштаб у этих признаков разный, давайте исправим это."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I0DMbY5yzmAn"
   },
   "source": [
    "Нормализуйте признаки так, чтобы среднее значение в каждом столбце было $\\sim 0$, а стандартное отклонение $\\sim 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "by_okjU0zmAo"
   },
   "outputs": [],
   "source": [
    "data_train_normalized = (data_train - data_train.mean(axis=0)) / data_train.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hp-JLFDRzmAr"
   },
   "source": [
    "Проверьте средние и диспресию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUFuFHrmzmAs"
   },
   "outputs": [],
   "source": [
    "data_train_normalized.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cOa4B4IXzmAu"
   },
   "outputs": [],
   "source": [
    "data_train_normalized.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmFr422fzmAy"
   },
   "source": [
    "Попробуем теперь запустить регрессию с теми же параметрами `lr` и `num_iter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0QbFEV7ZzmAy"
   },
   "outputs": [],
   "source": [
    "losses = grad_descent(data_train_normalized, result_train, 1e-2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tVruzNtAzmA0"
   },
   "outputs": [],
   "source": [
    "W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vn4NXpo4zmA2"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "de8U-uSYzmA4"
   },
   "source": [
    "Мы видим, что теперь коэффициенты по модулю не огромны, градиентный спуск не взрывается и лосс стабилен!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-wLnnMekzmA5"
   },
   "source": [
    "Посмотрим на полученную плоскость:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZGEhS_nvzmA5"
   },
   "outputs": [],
   "source": [
    "# %matplotlib osx\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.scatter(xs=data_train[:, 0], ys=data_train[:, 1], zs=result_train, c='red')\n",
    "\n",
    "X, Y = np.meshgrid(np.linspace(-1, 10, 100), np.linspace(-1, 150, 100))\n",
    "ax.plot_surface(X, Y, 0.43 * X + 0.5 * Y + 0.67, color='green', alpha=0.3)\n",
    "\n",
    "X, Y = np.meshgrid(np.linspace(-1, 10, 100), np.linspace(-1, 150, 100))\n",
    "# не забудьте нормализовать X и Y тоже\n",
    "ax.plot_surface(X, Y, W[0] * (X - X.mean()) / X.std() + W[1] * (Y - Y.mean()) / Y.std() + b, color='blue', alpha=0.3)\n",
    "\n",
    "ax.view_init(elev=60)\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LnJmxSlCzmA-"
   },
   "source": [
    "### Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0NXw5MtzmA_"
   },
   "source": [
    "Помимо \"сырой\" линейной регрессии часто используют линейную регрессию с регуляризацией - **Lasso** или **Ridge** регрессию. Они отличаются только типом \"штрафа\" за большие веса: учитывать модули (**Lasso**) или квадраты весов (**Ridge**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "99ajXzw8zmA_"
   },
   "source": [
    "Учитывая, что наш лосс в этой задаче - Mean Squared Error (MSE), в случае Ridge-регрессии будет:  \n",
    "\n",
    "$$\n",
    "Loss = MSE = \\sum (pred_i-y_i)^2 + \\alpha*\\sum W_i^2\n",
    "$$  \n",
    "\n",
    "А в случае Lasso-регрессии будет:  \n",
    "\n",
    "$$\n",
    "Loss = MSE = \\sum (pred_i-y_i)^2 + \\alpha*\\sum |W_i|\n",
    "$$  \n",
    "\n",
    "Здесь $\\alpha$ - заранее задаваемый гиперпараметр. Это вес, с которым второе слагаемое будет влиять на лосс."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-_w_qDo2zmBA"
   },
   "source": [
    "Добавление регуляризации, как правило, помогает бороться с **переобучением**.\n",
    "\n",
    "Подробнее об этом можно почитать [тут](http://www.machinelearning.ru/wiki/index.php?title=Лассо) и [тут](http://www.machinelearning.ru/wiki/index.php?title=Ридж-регрессия)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полезные ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-5_oOJCzmBB"
   },
   "source": [
    "1. Открытый курс по машинному обучению - https://habr.com/company/ods/blog/323890/\n",
    "\n",
    "2. Если вам интересно математическое обоснование всех методов, рекомендуем ознакомиться с этой книгой - https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[homework]gradient_linear_model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
